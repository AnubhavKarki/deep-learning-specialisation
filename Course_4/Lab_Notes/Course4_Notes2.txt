Week 2: CNNs Case Studies

Classic Neural Network Architectures:
    1. LeNet-5:
        Developed by: Yann LeCun et al., 1998
        Purpose: Designed for handwritten digit recognition (MNIST dataset)
        Significance: One of the first successful CNN architectures demonstrating the power of convolutional layers for image recognition

        Architecture Overview:
            Layer	Type	            Output Size             	Description
            Input	Image	            32×32 grayscale	    Original images were 28×28, padded to 32×32
            C1	    Convolution	        28×28×6	            6 filters of size 5×5, stride 1, no padding; feature extraction
            S2	    Subsampling	        14×14×6	            Average pooling (2×2), stride 2; reduces spatial size
            C3	    Convolution	        10×10×16	        16 filters of size 5×5, complex features
            S4	    Subsampling	        5×5×16	            Average pooling (2×2), stride 2
            C5	    Convolution	        1×1×120	            Fully connected convolution with 120 feature maps
            F6	    Fully Connected	    84 neurons	        Classic fully connected layer
            Output	Fully Connected	    10 neurons	        10-class output (digits 0–9)

        Key Points:
            Uses tanh activation (instead of ReLU, common today)
            Subsampling layers (S2, S4) perform average pooling to reduce feature map size and computation
            Learned filters capture edges, simple shapes in early layers, and complex digit parts deeper in the network
            Total parameters: ~60,000 (small compared to modern CNNs)
            Trained with backpropagation and gradient descent

        Why LeNet-5 Matters:
            Pioneered weight sharing and local connectivity concepts
            Demonstrated CNNs could outperform traditional ML methods on image tasks
            Inspired architectures like AlexNet, VGG, ResNet

    2. AlexNet:
        Developed by: Alex Krizhevsky, Ilya Sutskever, Geoffrey Hinton, 2012
        Purpose: Won ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, revolutionizing deep learning for computer vision
        Significance: First deep CNN to successfully scale to large datasets and complex images, showing the power of GPUs for training

        Architecture Overview:
            Layer	        Type	        Output Size	            Description
            Input	        Image	        227×227×3	        RGB color image
            Conv1	        Convolution	    55×55×96	        96 filters of size 11×11, stride 4, padding 0
            MaxPool1	    Max Pooling	    27×27×96	        3×3 window, stride 2
            Conv2	        Convolution	    27×27×256	        256 filters of size 5×5, padding 2
            MaxPool2	    Max Pooling	    13×13×256	        3×3 window, stride 2
            Conv3	        Convolution	    13×13×384	        384 filters of size 3×3, padding 1
            Conv4	        Convolution	    13×13×384	        384 filters of size 3×3, padding 1
            Conv5	        Convolution	    13×13×256	        256 filters of size 3×3, padding 1
            MaxPool5	    Max Pooling	    6×6×256	            3×3 window, stride 2
            FC6	            Fully Connected	4096 neurons	    Dense layer
            FC7	            Fully Connected	4096 neurons	    Dense layer
            FC8	            Fully Connected	1000 neurons	    Output layer (ImageNet classes)

        Key Innovations:
            ReLU activation: Enabled faster training compared to tanh or sigmoid
            Dropout: To reduce overfitting in fully connected layers
            Data augmentation: To artificially expand training data, improving generalization
            GPU training: Leveraged parallelism to train deep networks efficiently
            Local response normalization (LRN): To encourage competition among neuron outputs

        Impact:
            Reduced top-5 error rate on ImageNet from ~26% to 15.3%
            Sparked massive interest and research in deep CNNs
            Foundation for modern architectures like VGG, ResNet, EfficientNet

    3. VGG-16:
        Developed by: Visual Graphics Group (VGG) at Oxford, by Karen Simonyan & Andrew Zisserman
        Published: 2014, ILSVRC runner-up
        Goal: Show that depth (more layers) with small filters improves performance

        Architecture Highlights:
            Input: 224×224×3 RGB image
            All convolution filters: 3×3, stride = 1, padding = 1
            All max pooling: 2×2, stride = 2
            Uses only ReLU (no LRN, no fancy tricks like AlexNet)
            Fully connected layers at the end, then softmax

        Layer-wise Structure:
            Block	            Layers	                                Output Size
            Input		                                                224×224×3
            Conv Block 1	    Conv3-64 → Conv3-64 → MaxPool	        112×112×64
            Conv Block 2	    Conv3-128 → Conv3-128 → MaxPool	        56×56×128
            Conv Block 3	    Conv3-256 → Conv3-256 → Conv3-256 → MP	28×28×256
            Conv Block 4	    Conv3-512 → Conv3-512 → Conv3-512 → MP	14×14×512
            Conv Block 5	    Conv3-512 → Conv3-512 → Conv3-512 → MP	7×7×512
            FC1	                Fully Connected (4096)	                4096
            FC2	                Fully Connected (4096)	                4096
            FC3	                Fully Connected (1000) → Softmax	    1000 classes

        Key Notes:
            3×3 filters are stacked to simulate larger receptive fields (e.g., 3 conv layers = 7×7 effective)
            Max pooling shrinks spatial dimensions to reduce computation and allow deeper layers
            Parameters: ~138 million — heavy for modern standards
            No batch norm originally
            Highly uniform architecture: Easy to implement and modify

        Why It Mattered:
            Set new benchmark for simplicity and depth
            Widely adopted for transfer learning & feature extraction
            Became a backbone for many other models and tasks

    4. ResNet (Residual Networks)
        Developed by: Kaiming He et al., Microsoft Research
        Published: 2015 (Won ILSVRC 2015)

        Key Innovation: Residual Connections (Skip Connections)
        Purpose: Enables training of very deep networks (34, 50, 101, 152+ layers) by solving the vanishing gradient problem and degradation problem.

        Residual Block - Core Idea:
            Instead of learning the full mapping H(x), the network learns the residual F(x) = H(x) - x, so:
              H(x) = F(x) + x

            This is implemented by adding the input x directly to the output of a few layers:
              Output = x + F(x)

        Residual Block Structure (Bottleneck version):
            Input → Conv(1x1) → BN → ReLU → Conv(3x3) → BN → ReLU → Conv(1x1) → BN → + Input → ReLU → Output

        Why It Works:
            Gradient Flow: Gradients can pass through the skip connections during backprop.
            Identity Mapping: If optimal mapping is identity, it's easier to learn F(x) = 0.
            Optimization: Helps train very deep nets without performance degradation.

        Architecture Example: ResNet-50
            Stage       Layers	                                    Output Shape
            Input		                                            224×224×3
            Conv1       7×7 conv, stride 2 + max pooling	        112×112×64
            Conv2_x	    3 × [1×1, 64 → 3×3, 64 → 1×1, 256]	        56×56×256
            Conv3_x	    4 × [1×1, 128 → 3×3, 128 → 1×1, 512]	    28×28×512
            Conv4_x	    6 × [1×1, 256 → 3×3, 256 → 1×1, 1024]	    14×14×1024
            Conv5_x	    3 × [1×1, 512 → 3×3, 512 → 1×1, 2048]	    7×7×2048
            Output	    AvgPool → FC → Softmax (1000 classes)	    1×1×1000

        ResNet-18/34: Use simpler 2-layer residual blocks

        ResNet-50/101/152: Use bottleneck 3-layer blocks for deeper nets

        Mathematical Insight (Backprop):
            If y = F(a) + a,
            then gradient:
            ∂L/∂a = ∂L/∂y * (1 + ∂F/∂a)
            → gradient always flows, helping deep networks converge.

        Summary:
            ResNet learns small changes instead of full transformations.
            Makes deep models easier to train and more accurate.
            Solves degradation and vanishing gradient issues.
            Foundation for later models like DenseNet, EfficientNet, Transformers.

Network in Network (NiN)
    Proposed by: Lin, Chen, and Yan (2013)
    Paper: Network In Network

    Motivation:
    Classic CNNs (e.g., LeNet, AlexNet) use linear filters (e.g., 3×3 convolutions) followed by non-linear activations. But these may be limited in 
    capturing complex patterns. NiN replaces linear filters with small neural networks to learn more abstract and discriminative features.

    Key Concept: MLPConv Layers
        Instead of a simple conv layer (Conv → ReLU), NiN uses a multi-layer perceptron (MLP) after convolution at each location.

    Practically implemented using:
        Conv(n×n) → ReLU → Conv(1×1) → ReLU → Conv(1×1) → ReLU

    This allows:
        1. More complex feature transformations
        2. Non-linearity between channels
        3. Channel-wise interactions at each spatial location

1x1 Convolutions (aka Pointwise Convolution)
    Popularized by: NiN and later used extensively in GoogLeNet (Inception) and ResNet bottlenecks
    What it is:
        A 1×1 convolution filter slides over the input.
        It does not consider spatial neighborhood but processes all channels at each (x,y) location.
        Acts like a fully connected layer over channels at each pixel.

    Why Use 1x1 Convolutions?
        Dimensionality Reduction:
            Reduce number of channels while keeping spatial dimensions same
              (e.g., 256→64 before a 3×3 conv, reducing computation)

        Non-linear combinations across channels:
            Adds depth and abstraction without increasing spatial complexity
              (e.g., from simple edge maps to complex textures)

        Computational Efficiency: Smaller models, faster training (used in MobileNet, SqueezeNet)
            Used in:
                NiN (for MLP-like depth)
                GoogLeNet (Inception modules): squeeze & expand blocks
                ResNet Bottlenecks: 1×1 → 3×3 → 1×1
                DenseNet, MobileNet, EfficientNet

            Example – Simple NiN Block:
                Input → Conv(5x5, 192) → ReLU  
                    → Conv(1x1, 160) → ReLU  
                    → Conv(1x1, 96) → ReLU  
                    → Max Pooling → Dropout
                Each 1×1 Conv layer mixes channel information and adds non-linearity.

Summary:
    NiN = Replace plain conv filters with mini neural nets (MLPConv)
    1×1 Convs = Efficient, channel-wise computation
    Great for reducing parameters, enhancing feature abstraction, and enabling deeper architectures without heavy compute

Inception Network (GoogLeNet):
    Proposed by: Szegedy et al., Google
    Paper: Going Deeper with Convolutions (2014)
    Won ILSVRC 2014 with Top-5 error: 6.67%

    Motivation:
        Deeper networks (like VGG) increase parameters and computation.
        A major challenge: choosing the right filter size (1×1, 3×3, 5×5).
        Instead of choosing one, Inception Module allows multiple filter sizes to operate in parallel.
        This helps the network capture features at different scales and maintain computational efficiency.

    Inception Module – Core Building Block
        Each module applies parallel operations on the input:
                            1x1 conv
                            │
                            ▼
            Input ──┬──→ 1x1 conv ──┐
                │               │
                ├──→ 1x1 conv → 3x3 conv ─┐
                │                         ├─→ Concatenate → Output
                ├──→ 1x1 conv → 5x5 conv ─┘
                │
                └──→ 3x3 max pool → 1x1 conv

    Why Use 1×1 Convolutions in Inception?
        Dimensionality reduction:
          Reduce number of input channels before costly 3x3 and 5x5 convolutions.
          This drastically reduces computation.

        Example:
            If input has 256 channels, a 5x5 conv would have 256×5×5×n params.
            A 1x1 conv first reduces it to, say, 32 → then 5x5 only applies on 32 channels.
                Adds non-linearity between convs
                Efficient bottleneck layer design

    Structure of GoogLeNet (Inception v1):
        Total of 22 layers deep (27 with pooling)
        No fully connected layers at the end – instead uses global average pooling
        Number of parameters ~5M (much less than VGG-19 with 144M+)

    Overall pipeline:
        Input (224x224x3)
        ↓
        Conv 7x7, stride 2 → Max Pool
        ↓
        Conv 1x1 → Conv 3x3 → Max Pool
        ↓
        Inception Module x9 (3a → 5b)
        ↓
        Global Average Pooling (1x1x1024)
        ↓
        Dropout
        ↓
        Linear FC (1000 outputs)
        ↓
        Softmax

    Variants:
        Inception v1 (GoogLeNet):
            Introduced core idea of the Inception module
            Used 1×1 convolutions for bottleneck
            No FC layer, used Global Average Pooling before classification

        Inception v2 / v3:
            Introduced Factorization of convolutions:
              e.g., replace 5×5 with two 3×3 convs → faster, fewer params
                  or replace n×n with n×1 + 1×n to reduce cost
            Added Batch Normalization
            Improved training and generalization

        Inception v4 / Inception-ResNet:
            Combine Inception modules with Residual Connections
            Best of both worlds: multiscale + residual learning
            Allows even deeper networks to train efficiently

    Why Inception Works So Well:
        Multi-scale Feature Learning:
          Different kernel sizes capture different levels of detail

        Sparsity & Efficiency:
          Instead of dense connections, use parallel sparse filters
          Made feasible through clever module design

        Reduced Parameters:
          Thanks to 1x1 bottlenecks and no FC layers

        Global Average Pooling:
          Reduces overfitting
          Replaces massive FC layers with a single averaged feature vector

    Summary:
        Inception is a smart, modular deep CNN
        Learns multiscale features efficiently
        Introduced bottleneck layers with 1x1 convs
        Achieves high accuracy with fewer parameters
        Later versions combine residuals for deeper, better models

MobileNet Architecture
    Developed by: Google
    Paper: MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications (Howard et al., 2017)

    Goal:
        Build efficient and lightweight CNNs for mobile and embedded devices
        Minimize model size, latency, and computation without sacrificing too much accuracy

    Key Idea: Depthwise Separable Convolution
        Instead of using a standard convolution, which is computationally expensive, MobileNet uses:
        Depthwise Separable Convolution = Depthwise Convolution + Pointwise Convolution
    Standard Convolution Recap:
        Let’s say:
            Input: 32×32×3 (H×W×Channels)
            Filter: 3×3×3 (spatial size × depth)
            Suppose 64 filters → Output: 30×30×64

        Total Parameters: 3×3×3×64 = 1,728
        Each filter spans all input channels.

        Computational cost:
            H×W×Cin×K×K×Cout

    Depthwise Separable Convolution
        Step 1: Depthwise Convolution
            Applies one filter per input channel
            Each filter is 3×3×1
            So for 3 channels → 3 filters of 3×3×1

        No mixing between channels; Output still has 3 channels, but now each channel is spatially filtered.

        Step 2: Pointwise Convolution
            Now apply a 1×1 convolution to combine these filtered channels
            1×1×3 filter × 64 filters = combines depth into 64 channels

    Parameter Comparison
        Input: 32×32×3; 64 filters, each 3×3 in standard conv

        Standard Conv:
            Params = 3×3×3×64 = 1,728
            Mult-adds = 32×32×1,728 = 1.77M

        Depthwise Separable:
            Depthwise: 3×3×1 × 3 = 27
            Pointwise: 1×1×3 × 64 = 192
            Total = 27 + 192 = 219 parameters
            Mult-adds = much lower (~8–9× fewer)
            → ~8-9x fewer computations & parameters

    MobileNet Architecture Overview:
        Conv Layer → BatchNorm → ReLU6 used throughout.

    Typical Flow (MobileNet v1):
            Input (224×224×3)
            ↓
            Conv 3x3, stride 2
            ↓
            Depthwise Conv 3x3 → Pointwise Conv 1x1, ReLU6
            ↓
            (Repeated with stride 1 or 2 to downsample)
            ↓
            Global Average Pooling
            ↓
            FC Layer (e.g., 1000 classes)
            ↓
            Softmax

    Key Hyperparameters:
        Width Multiplier (α):
            Scales the number of channels:
                α = 1: standard MobileNet
                α = 0.5: 50% fewer channels → smaller model

        Resolution Multiplier (ρ):
            Downsamples image input
            Input: 224×224 → 192×192 or 160×160

    Trade-off between speed vs accuracy
        MobileNet Variants:
            MobileNet v1 (2017):
              Introduced Depthwise Separable Conv, very lightweight

            MobileNet v2 (2018):
              Introduced inverted residuals and linear bottlenecks
              Used skip connections like ResNet for better gradient flow

            MobileNet v3 (2019):
              Used Neural Architecture Search (NAS) for even better efficiency
              Included squeeze-and-excitation blocks and swish activations

    Why Depthwise Separable Conv is Great:
        Massively reduces parameters and computation
        Preserves accuracy reasonably well
        Perfect for edge devices, IoT, phones

    Summary:
        MobileNet is built on Depthwise Separable Convolutions
        Instead of dense convs, it splits into lightweight operations
        Uses fewer parameters, less memory, and less computation
        Still performs strongly on classification and detection tasks
        MobileNet v2 and v3 improve on the basic structure with residuals and NAS

What is bottleneck and why it is used:
    A bottleneck in deep neural networks refers to a layer or structure that intentionally reduces the number of channels (features) to a smaller 
    dimension before expanding it again. It’s a technique used to reduce the computational cost and improve efficiency without losing important 
    information. In architectures like ResNet, the bottleneck block first uses a 1×1 convolution to reduce dimensionality, then performs the heavy 
    computation (like 3×3 convolution), and finally restores the original dimensions with another 1×1 convolution. In MobileNet v2, the concept is 
    flipped — first expanding, then compressing — called an inverted bottleneck. Bottlenecks help make very deep networks faster and lighter while 
    maintaining or even improving accuracy. They also encourage the network to learn compact, essential features, which improves generalization and 
    reduces overfitting.

EfficientNet – Notepad Style Notes
    Developed By: Google AI (Mingxing Tan and Quoc V. Le)
    Published: 2019
    Paper Title: "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"
    Goal: To design a family of models that are more accurate and efficient (fewer parameters, faster inference) by optimizing how the network scales.

    Compound Scaling
        Traditional models scale arbitrarily:
            Deeper (more layers)
            Wider (more channels per layer)
            Higher resolution (larger input images)

        EfficientNet scales all three together using a fixed compound coefficient (ϕ):
            depth: d = α^ϕ
            width: w = β^ϕ
            resolution: r = γ^ϕ
            subject to constraint: α * β² * γ² ≈ 2
            (α, β, γ are constants determined via grid search)

    Base Architecture: EfficientNet-B0
        Starts from a small but well-tuned base network (MobileNetV2-like)
        Builds larger versions B1 to B7 using compound scaling
        Each version increases accuracy and size in a principled way
        Building Block: MBConv (Mobile Inverted Bottleneck Convolution)

    Same as MobileNetV2 blocks:
        1x1 conv (expansion)
        Depthwise 3x3 conv (spatial filtering)
        1x1 conv (projection)
        Squeeze-and-Excitation (SE) block added for channel-wise attention
        Skip connection if input and output dimensions match
        EfficientNet uses MBConv6 (expansion ratio of 6) heavily

    Architecture (EfficientNet-B0)

    Stage	Operator	                Resolution	            Channels	    Layers
    1	    Conv 3x3	                224×224	                32	            1
    2	    MBConv1, k3x3	            112×112	                16	            1
    3	    MBConv6, k3x3	            112×112	                24	            2
    4	    MBConv6, k5x5	            56×56	                40	            2
    5	    MBConv6, k3x3	            28×28	                80	            3
    6	    MBConv6, k5x5	            14×14	                112	            3
    7	    MBConv6, k5x5	            14×14	                192	            4
    8	    MBConv6, k3x3	            7×7	                    320	            1
    9	    Conv 1x1 + Pool + FC	    7×7	                    1280	        -

    ReLU6 replaced with Swish activation (smooth, non-linear)

    EfficientNet Variants
        EfficientNet-B0: baseline
        B1 – B7: progressively larger using compound scaling
        EfficientNet-L2: very large model used for state-of-the-art results
        Tiny models like EfficientNet-Lite for mobile and edge devices

    Advantages:
        State-of-the-art accuracy with far fewer parameters
        Efficient inference: faster and less memory intensive
        Compound scaling ensures better use of compute resources
        Scales elegantly to meet different resource/accuracy needs

    Summary:
        EfficientNet uses a principled compound scaling method to scale depth, width, and resolution together.
        Uses MBConv blocks with SE and Swish activation for powerful and lightweight computation.
        Achieves better accuracy with 10x fewer parameters and FLOPS compared to older models like ResNet and Inception.
        Became a foundation for follow-ups like EfficientNetV2, Noisy Student Training, and Vision Transformers later.

Object Classification and Localization:
    Object classification is the foundational task in computer vision where a model learns to recognize and assign a category label to an entire image. 
    For example, given an image of a dog, the model outputs a label such as “dog.” This task assumes there is a single dominant object in the image, 
    and it doesn’t concern itself with where in the image the object is located. While useful in simple use cases, this approach fails in complex, 
    real-world scenarios where images may contain multiple objects at various locations and scales.

    To tackle this limitation, the concept evolved into classification with localization, where the goal is not just to predict what is in the image, 
    but also where it is. In this paradigm, the model outputs both the class label and the spatial coordinates of the object via a bounding box. 
    The bounding box helps to specify the object’s location by drawing a rectangle around it, and this is crucial for applications like 
    autonomous vehicles, medical diagnostics, and surveillance.

    Example Setup:
        Let’s define the output label vector y for a single object in the image. The output vector contains:
            y = [pc, bx, by, bh, bw, c1, c2, ..., cn]
        Where:
            pc = Probability/confidence that there is an object in the image.
            bx, by = Coordinates of the center of the bounding box (relative to image size).
            bh, bw = Height and width of the bounding box (also normalized).
            c1 to cn = One-hot encoded class labels for n possible classes.

        Example:
            For a dog-vs-cat classifier:
                y = [1, 0.3, 0.5, 0.4, 0.6, 1, 0]
            This means: object is present, center is at (0.3, 0.5), height 0.4, width 0.6, and it's a dog (not a cat).

    How Bounding Boxes Are Formed:
        The bounding box is defined by its center (bx, by) and its size (bh, bw). These values are normalized between 0 and 1 based on the image dimensions.
        Example: if an image is 100x100 pixels and bx = 0.3, the actual x-coordinate is 30 pixels.

    From the center, you calculate:
        x_min = (bx - bw / 2) * image_width
        y_min = (by - bh / 2) * image_height
        x_max = (bx + bw / 2) * image_width
        y_max = (by + bh / 2) * image_height

        Then you draw the box from (x_min, y_min) to (x_max, y_max).

    Loss Function:
        The loss is a combination of three parts: localization loss, confidence loss, and classification loss.
        Total_Loss = λ_coord * Loc_Loss + λ_obj * Conf_Loss + λ_cls * Cls_Loss

        Where:
            Loc_Loss  = (bx - bx_hat)^2 + (by - by_hat)^2 + (bh - bh_hat)^2 + (bw - bw_hat)^2
            Conf_Loss = (pc - pc_hat)^2
            Cls_Loss  = cross_entropy(predicted_class, true_class)

    You can tune how much weight each part has using λ_coord, λ_obj, and λ_cls. These are hyperparameters to balance the importance of box accuracy, 
    object confidence, and classification correctness.

    This framework is the basis for more advanced object detection models like YOLO and SSD, which apply the same principles across grids and 
    anchor boxes for detecting multiple objects efficiently. But even at this base level, classification with localization is a powerful upgrade 
    from plain image classification.

Landmark Detection:
    Landmark detection (also known as keypoint detection or facial landmark localization) is the task of identifying specific points of interest (landmarks) 
    on an object within an image, typically on human faces, bodies, or even hands.

    While object detection predicts bounding boxes for entire objects, landmark detection predicts coordinates of key structural points on the object.
    For example, in face analysis, these points could be the tip of the nose, corners of the eyes, mouth, chin, etc.

    This task is crucial in many real-world applications:
        Face recognition and alignment
        Emotion and gaze estimation
        AR filters and face tracking
        Medical image analysis

    Landmark Detection Output Vector:
        Assuming you are detecting n landmarks, and each has an (x, y) coordinate (normalized between 0 and 1):
            y = [x1, y1, x2, y2, ..., xn, yn]
        For example, if you're detecting 5 facial landmarks (left eye, right eye, nose, left mouth corner, right mouth corner):
            y = [x1, y1, x2, y2, x3, y3, x4, y4, x5, y5]
        Each coordinate is relative to image width and height.

    Loss Function:
        The loss function is a simple Mean Squared Error (MSE) or L2 loss between predicted and true coordinates:
            Loss = (1 / n) * Σ (xi - xi_hat)^2 + (yi - yi_hat)^2
            Where:
                xi, yi are the true coordinates
                xi_hat, yi_hat are the predicted coordinates
                n is the number of landmarks

        If regularization is needed:
            Total_Loss = Loss + λ * regularization_term

    Example:
        Suppose you have a 64x64 image of a face and want to detect 3 landmarks: left eye, right eye, and nose.

        True landmarks (normalized):
            y_true = [0.30, 0.40, 0.70, 0.40, 0.50, 0.60]

        Predicted landmarks:
            y_pred = [0.32, 0.39, 0.68, 0.41, 0.52, 0.58]

        Then the loss is:
            Loss = (0.30 - 0.32)^2 + (0.40 - 0.39)^2 +
                (0.70 - 0.68)^2 + (0.40 - 0.41)^2 +
                (0.50 - 0.52)^2 + (0.60 - 0.58)^2
                = 0.0004 + 0.0001 + 0.0004 + 0.0001 + 0.0004 + 0.0004
                = 0.0018

    How It Works in Practice:
        A CNN takes in the image and outputs a flat vector of 2*n values.
        You train it using supervised learning where the target is the vector of ground truth keypoints.
        It does not use bounding boxes; it directly regresses keypoints.

    Common Models Used:
        Simple CNNs (shallow for small tasks)
        Hourglass Networks
        Stacked DenseNet
        MobileNet for real-time applications
        Heatmap-based models (instead of regression)

Sliding Window:
    The sliding window technique was one of the earliest approaches to object detection. The core idea is simple: we take a fixed-size 
    window and "slide" it across the image at different positions (and scales), and at each step, we classify whether the object of interest 
    is present in that window.

    This approach essentially converts object detection into a classification problem. For example, to detect faces in an image, we slide 
    a 64x64 window over every region of the image, feeding it into a classifier (like an SVM or CNN) that predicts whether that region 
    contains a face or not.

    To handle objects of different sizes, the image is also rescaled multiple times, and the sliding window is applied at each scale 
    known as multi-scale detection.

    Why Is Sliding Window Computationally Expensive?
        Massive Number of Windows:
            For a single image of size 256x256, using a window of size 64x64 and a stride of just 4 pixels, you'd have to evaluate thousands of windows. 
            Now if you consider multiple scales, this easily multiplies to tens or hundreds of thousands of windows per image.

        Each Window Requires Inference:
            Every window must pass through a classifier or CNN. Even if it’s a small model, doing that thousands of times for one image is expensive, 
            especially in real-time applications.

        No Context Awareness:
            Sliding window treats each patch independently. It doesn’t leverage the full spatial or semantic context of the image. That means redundant 
            computations, especially if adjacent windows overlap heavily (which they do).

    Why Is It Not Preferred Today?
        The sliding window approach is now considered inefficient and outdated, mainly due to the rise of more optimized deep learning architectures that:
            Learn to predict locations directly (like in YOLO or SSD).
            Use region proposal networks (like in Faster R-CNN).
            Leverage shared computation via convolutional layers, avoiding the need to reprocess overlapping patches.

    In short, rather than brute-force checking every patch, modern object detectors use end-to-end networks that both localize and 
    classify objects efficiently. These methods are not only faster but also more accurate and scalable.
