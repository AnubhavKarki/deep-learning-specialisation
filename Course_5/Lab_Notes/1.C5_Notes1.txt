Course 5: Week 1

Sequence Models:
    Sequence Models are models where the input, output, or both are sequences. Examples include:
        Music generation (input: initial melody, output: generated sequence of notes)
        Speech recognition (input: audio signal, output: text transcript)
        Sentiment classification (input: text, output: sentiment label)
        DNA sequence analysis (input: DNA base sequence, output: classification or prediction)

    These can be portrayed as supervised learning problems because for each training example, we have an input sequence x and a corresponding target 
    sequence or label y. The model learns a mapping from x to y by minimizing a loss function based on training examples.

    Notation:
        Example sentence:
            x: Harry and Hermione invented a new spell.
            y: 1 0 1 0 0 0 0 (NER - Named Entity Recognition labels)

            Each word in x is a feature to be learned:
            x<1>, x<2>, x<3> ...
            Similarly for the output sequence:
            y<1>, y<2>, y<3> ...

            T_x = total number of time steps in the input sequence (length of x)
            T_y = total number of time steps in the output sequence (length of y)

        Note: T_x and T_y do not need to be equal.
        Reason: In some tasks, the input and output sequence lengths are different. Example: machine translation (English sentence 
                length ≠ French sentence length).

        Indexing in datasets:
            x(i)<t> = t-th value (word, token, etc.) of the i-th training example input sequence.
            T_x(i) = length of the i-th training example's input sequence.
            y(i)<t> = t-th value of the i-th training example's output sequence.
            T_y(i) = length of the i-th training example's output sequence.

    Representing words:
        Given the sentence x, we compare each word to a predefined dictionary (vocabulary) of size V (e.g., 10,000 words).
        Each word is represented as a one-hot vector of shape (V, 1) where the entry corresponding to that word in the dictionary is 1 and all 
        other entries are 0.

        Example:
            If the dictionary index for "Harry" is 407, then its one-hot encoding is a vector of zeros except the 407-th position, which is 1.
            If a word in the input sentence does not exist in the dictionary, it is replaced by a special <UNK> token (Unknown). This happens to 
            handle rare or unseen words during training and inference, ensuring the model can still process the sequence without breaking.
    
    Why not a standard network?
        In sequence problems, the input and output lengths can vary across examples. For example, T_x != T_y. A standard feedforward network requires 
        fixed input and output dimensions, which is not flexible for sequences.

        A standard network does not share learned features across different positions in the text. For example, if the word "Harry" appears in 
        position 1 vs. position 10, the model treats them as completely unrelated.

        An RNN overcomes both these problems:
            It can handle varying sequence lengths.
            It shares parameters across time steps, so patterns learned in one position can be applied elsewhere.
    
    Recurrent Neural Network (RNN):
        An RNN processes sequences one element at a time while maintaining a hidden state a<t> that captures information about all the previous inputs.
        At each time step t, the RNN takes the current input x<t> and the previous hidden state a<t-1> to compute the new hidden state a<t>. 
        This hidden state is then used to produce an output y<t>.

        Example structure:
            x<1> --> [RNN Cell] --> y<1>
                    ↑
            x<2> --> [RNN Cell] --> y<2>
                    ↑
            x<3> --> [RNN Cell] --> y<3>
            ...
    
    Context capture and weakness:
        When predicting based on x<3>, the RNN uses both x<3> and the past context from x<1>, x<2>.
        However, standard RNNs only use information from the past, not from future inputs.

        Example:
            Sentence: "He said, Teddy Roosevelt was a good President"
            If we want to decide if "Teddy" is the president, the earlier part doesn't give enough information. The later part "was a good President" 
            gives the real clue — but a standard RNN moving left to right cannot use this future context.
    
    Forward propagation in RNN:
        For each time step t:
            a<1> = g(W_aa * a<0> + W_ax * x<1> + b_a)
            y<1> = g(W_ya * a<1> + b_y)

            Where:
                g is usually tanh for hidden state, and softmax or sigmoid for output.
                W_aa is the weight matrix for hidden-to-hidden connections (previous state to current state).
                W_ax is the weight matrix for input-to-hidden connections (input to current state).
                W_ya is the weight matrix for hidden-to-output connections.

        Meaning of parameters:
            W_aa (shape: n_a x n_a) — maps previous hidden state a<t-1> to the new hidden state.
            W_ax (shape: n_a x n_x) — maps current input x<t> to the hidden state.
            W_ya (shape: n_y x n_a) — maps hidden state to output.

    General equations:
        a<t> = g(W_aa * a<t-1> + W_ax * x<t> + b_a)
        y-hat<t> = softmax(W_ya * a<t> + b_y)

        Where:
            y-hat<t> is the prediction at time step t.

    Simplified RNN notation:
        We can concatenate W_aa and W_ax into one matrix W_a by stacking them vertically:
            a<t> = g(W_a * [a<t-1>, x<t>] + b_a)

            Here:
                [a<t-1>, x<t>] means concatenating the vectors a<t-1> and x<t> into a single column vector of shape (n_a + n_x, 1).
                W_a will then have shape (n_a, n_a + n_x).

        Example of dimensions:
            Let:
                n_x = 3 (input feature size per time step)
                n_a = 5 (hidden state size)
                n_y = 2 (output size)

            Shapes:
                W_ax: (5, 3)
                W_aa: (5, 5)
                W_ya: (2, 5)

            If we stack W_aa and W_ax horizontally:
                W_a: shape (5, 8)   # because n_a = 5, and n_a + n_x = 8
                [a<t-1>, x<t>]: shape (8, 1)

            Matrix multiplication:
                (5, 8) x (8, 1) = (5, 1)   # gives a<t>
Backpropagation:
    Forward Propagation Recap:
        For each time step t:
            a<t> = g(W_aa * a<t-1> + W_ax * x<t> + b_a)
            y-hat<t> = softmax(W_ya * a<t> + b_y)

    Loss Function at a single time step:
        If we use cross-entropy loss:
            L<t> = - Σ_k y<t>_k * log( y-hat<t>_k )
        where k indexes over output classes.

    Overall Cost Function for one example:
        If the sequence length is T_y:
            L = Σ_{t=1}^{T_y} L<t>

    Overall Cost Function for the full training set:
        If we have m examples:
            J = (1/m) * Σ_{i=1}^m L^{(i)}

    What backpropagation needs to calculate:
        We need the gradients of the cost J with respect to all parameters:
            dW_aa, dW_ax, dW_ya
            db_a, db_y

        Also gradients with respect to hidden states da<t> (needed to propagate errors back through time).

    Variables updated during training:
        In each gradient descent step:
            W_aa := W_aa - α * dW_aa
            W_ax := W_ax - α * dW_ax
            W_ya := W_ya - α * dW_ya
            b_a  := b_a  - α * db_a
            b_y  := b_y  - α * db_y
            where α is the learning rate.

    Backpropagation in RNNs:
        Because the hidden state a<t> depends on a<t-1>, which depends on a<t-2>, and so on, the gradient of the cost with respect to earlier time steps requires 
        applying the chain rule across all time steps.

        We pass errors backward from the last time step all the way to the first, updating parameter gradients based on how much each step contributed to the loss.

        This process is called Backpropagation Through Time (BPTT) because the unrolled RNN looks like a deep feedforward network where each layer 
        corresponds to a time step.

    Why "Through Time"?
        The hidden activations a<t> are the most important values passed forward in RNNs.
        In BPTT, we pass the gradient signals backwards in time, flowing through all stored activations from the last step back to the first.
        This is necessary because the same parameters are reused at every time step, so their gradient is the sum of contributions from all steps.

    Different types of backpropagation algorithms used in RNNs:
        Full BPTT:
            Backpropagate through the entire sequence length.
            Computationally expensive for very long sequences.

        Truncated BPTT:
            Backpropagate through a fixed number of time steps (e.g., 20 steps) instead of the whole sequence.
            Reduces computation and helps with vanishing/exploding gradient problems.


    Types of RNN Architectures with Different Input and Output Lengths (T_x != T_y):
        One-to-One:
            This is a standard neural network where input and output are single values (T_x = T_y = 1). Not really an RNN but included for completeness.

        One-to-Many:
            The input is a single element (T_x = 1), and the output is a sequence (T_y > 1).
            Example: Image captioning — input is an image vector, output is a sentence describing the image.

            Formally:
                a<0> = initial hidden state (often zeros)
                a<1> = g(W_aa * a<0> + W_ax * x<1> + b_a)
                y-hat<1> = softmax(W_ya * a<1> + b_y)
                a<2> = g(W_aa * a<1> + b_a)       # no new input, just hidden state updated
                y-hat<2> = softmax(W_ya * a<2> + b_y)
                ...
                a<T_y> = g(W_aa * a<T_y-1> + b_a)
                y-hat<T_y> = softmax(W_ya * a<T_y> + b_y)
            Here, after the first input, the RNN generates outputs based only on its hidden state without additional inputs.

        Many-to-One:
            Input is a sequence (T_x > 1), but output is a single value (T_y = 1).
            Example: Sentiment classification — input is a sentence, output is one label (positive/negative).

            Formally:
                For t in 1 to T_x:
                a<t> = g(W_aa * a<t-1> + W_ax * x<t> + b_a)
                Output at last time step:
                    y-hat = softmax(W_ya * a<T_x> + b_y)
            Here, only the last hidden state is used to produce the final output.

        Many-to-Many:
            Input and output are sequences, but lengths can be different (T_x != T_y).
            Example: Machine translation — input is an English sentence, output is a French sentence (different lengths).

            There are two variants:
                Synchronous many-to-many (T_x = T_y): Input and output sequences are same length, like video classification frame by frame.
                Asynchronous many-to-many (T_x != T_y): Input and output lengths differ; the output sequence is generated step-by-step based on 
                the full input sequence.

        Sequence-to-Sequence (Seq2Seq) Model:
            This architecture uses two RNNs:
                Encoder RNN processes the input sequence into a context vector (final hidden state).
                Decoder RNN generates the output sequence based on the context vector and its own previous outputs.
                Formally, encoder:
                    For t in 1 to T_x:
                        a<t> = g(W_aa * a<t-1> + W_ax * x<t> + b_a)
                    Context vector:
                        c = a<T_x>
                    Decoder initializes hidden state with c:
                        For t in 1 to T_y:
                            s<t> = g(W_ss * s<t-1> + W_sy * y-hat<t-1> + b_s)
                            y-hat<t> = softmax(W_ys * s<t> + b_y)

                        Here, s<t> is the decoder hidden state at time t, and y-hat<t-1> is the previous output fed as input to the decoder 
                        (teacher forcing during training).

    Language Modeling:
        Language modeling is the task of predicting the next word in a sequence, given all the previous words. It assigns probabilities to sequences 
        of words and helps in many NLP tasks like text generation, speech recognition, and machine translation.

    Training Set:
        We use a large corpus of English text — millions of sentences — as our training data. The goal is to learn patterns and predict the likelihood
        of word sequences.

    Tokenization:
        Tokenization is the process of breaking down sentences into individual units called tokens, usually words or subwords. 
        For example:
            Sentence:
                Cats average 15 hours of sleep a day.
            Tokens:
                ["Cats", "average", "15", "hours", "of", "sleep", "a", "day", "."]

            Each token is then converted into a numerical representation (like an index) using a vocabulary dictionary.

        Example:
            Input sequence x: Cats average 15 hours of sleep a day.
            Output sequence y:
                y<1> = Cats
                y<2> = average
                ...
                Last token is a special <EOS> (end of sentence) token, marking sentence end.

    Handling Unknown Words:
        If a word in the input does not exist in the dictionary, it is replaced by a special <UNK> token (unknown), allowing the model to handle 
        rare or unseen words gracefully.

    Training the RNN Language Model:
        At each time step t, the input to the RNN is the previous token:
            x<t> = y<t-1>

        The model predicts the probability distribution of the next word y-hat<t> as a softmax over the vocabulary (e.g., 10,000 words):
            y-hat<t> = softmax(W_ya * a<t> + b_y)

        For example, at t=1, the RNN predicts the probability distribution for y<1> (e.g., "Cats"). We provide the correct previous word to the model 
        as input to the next step during training (teacher forcing).

    Chain Rule in Language Modeling:
        The overall probability of the sequence is the product of conditional probabilities:
            P(sentence) = P(y<1>) * P(y<2>|y<1>) * P(y<3>|y<1>, y<2>) * ... * P(y<T>|y<1>...y<T-1>)
        The RNN models this chain by predicting the probability distribution of each next word conditioned on all previous words.

    Softmax Output:
        At each time step t, the model outputs a probability distribution over the vocabulary:
            y-hat<t> = softmax(W_ya * a<t> + b_y)

        If the vocabulary size is V, then y-hat<t> is a (V, 1) vector where:
            y-hat<t>_k = exp(z_k) / Σ_{j=1}^V exp(z_j)
        and z = W_ya * a<t> + b_y.

        Loss at a Single Time Step:
            We use the cross-entropy loss between the predicted distribution y-hat<t> and the true one-hot label y<t>:
                L<t> = - Σ_{k=1}^V y<t>_k * log(y-hat<t>_k)

            Since y<t> is one-hot, this just picks the log-probability of the correct word:
                L<t> = - log( y-hat<t>_true )

        Loss for One Training Example:
            If the target sequence length is T_y:
                L = Σ_{t=1}^{T_y} L<t>

        Overall Cost for the Dataset:
            If we have m training examples:
                J = (1/m) * Σ_{i=1}^m Σ_{t=1}^{T_y^{(i)}} L<t>^{(i)}
    
    Sampling Novel Sequences:
        After training an RNN language model, we can generate new sequences by using the model’s predictions as inputs for future steps. We start 
        with a special <SOS> (start-of-sequence) token as the first input. The RNN processes this and outputs a probability distribution over the 
        vocabulary. Instead of always choosing the most probable word, we randomly sample a word from this distribution to introduce creativity. 
        This sampled word becomes the input for the next time step. We repeat this process, step-by-step, until the model produces an <EOS> 
        (end-of-sequence) token or reaches a predefined maximum sequence length. This method allows the model to generate varied and novel 
        outputs, although randomness can also lead to occasional errors or nonsensical phrases.

    Character-Level Language Models:
        In a character-level language model, each character is treated as a token. The vocabulary is relatively small — usually around 100 symbols 
        including letters, numbers, punctuation, and special characters. For example, the word “Cats” would be tokenized as ['C', 'a', 't', 's']. 
        This approach allows the model to generate any possible word, even those never seen during training, and it naturally handles rare words, 
        misspellings, and creative text. However, sequences become much longer because each word is broken down into multiple tokens. This makes 
        it harder for the model to capture long-term dependencies and increases training difficulty.

    Word-Level Language Models:
        In a word-level language model, each word is treated as a token. Vocabulary sizes can range from 10,000 to over 100,000 words. For example, 
        the sentence “Cats sleep a lot” would be tokenized as ['Cats', 'sleep', 'a', 'lot']. Word-level models capture semantic meaning more directly, 
        and sequences are shorter compared to character-level models, making it easier to capture context across a sentence. However, they cannot 
        generate new words outside their vocabulary and must use a special <UNK> token for unknown terms. Additionally, large vocabularies make the 
        softmax computation in the output layer more expensive.

    Vanishing Gradients in Basic RNNs:
        Basic RNNs struggle with learning long-term dependencies due to the vanishing gradient problem. This means that when training with 
        backpropagation through time (BPTT), the gradients used to update weights get smaller and smaller as they are propagated back through 
        many time steps. Eventually, these gradients approach zero, making it nearly impossible for the network to learn dependencies from 
        distant past inputs.

        For example, consider the sentences:
            “The cat, which already ate..., was full.”
            “The cats, which already ate..., were full.”

        The choice between “was” and “were” depends on whether the subject is singular (“cat”) or plural (“cats”).
        However, the phrase “which already ate...” can be arbitrarily long. For the RNN to correctly predict “was” or “were,” it needs to remember the 
        singular/plural information from many time steps ago.

        Basic RNNs typically have local influences, meaning they focus mostly on recent inputs. Because of vanishing gradients, the model fails to 
        propagate the important singular/plural signal across long gaps in the sentence. As a result, it cannot reliably capture long-range syntactic 
        dependencies required to make the correct verb agreement.

    Exploding Gradients:
        Exploding gradients happen when gradients become excessively large during backpropagation. Instead of shrinking, they grow exponentially as 
        they move backward through time steps, causing unstable training and huge parameter updates. This can make the model weights diverge and the 
        training process fail.

    Why Vanishing Gradients Are the Bigger Problem:
        While exploding gradients can often be controlled by techniques like gradient clipping (which caps the gradient magnitude), vanishing gradients are 
        harder to fix because they prevent learning from long-term dependencies altogether.

        Vanishing gradients cause RNNs to forget information from far in the past, which limits their effectiveness on tasks where context from many time 
        steps ago is critical. This fundamental problem is why basic RNNs have mostly been replaced by more advanced architectures like LSTMs and GRUs, 
        which use gating mechanisms to better preserve and manage long-range information.

Gated Recurrent Unit (GRU):
    GRUs are an improvement over basic RNNs, designed to solve the vanishing gradient problem and improve the ability to capture long-term dependencies.
    In basic RNNs, information tends to get lost as it passes through many time steps, making it difficult to relate distant words in a sequence. 
    GRUs add gating mechanisms that control when to keep information and when to update it, allowing the network to “remember” useful facts for much longer.

    GRU Units and the Memory Cell:
        In GRUs, we have a memory cell c<t> that stores the long-term information. It acts as a conveyor belt for information to travel across time steps with 
        minimal modification, helping preserve gradients.

        We also compute a candidate value c~<t> (read as “c-tilde”) that represents the new information we might want to store in the cell. 
        This candidate is calculated as:
            c̃<t> = tanh(W_c * [Γ_r ⊙ a<t-1>,  x<t>] + b_c)
        Here, Γ_r (Gamma_r) is the reset gate, which decides how much of the previous hidden state a<t-1> to use when creating the candidate.
        If Γ_r is close to 0, the network ignores most of the old hidden state for that step.

        Why Γ_r (Reset Gate) Matters:
            For sequences where the current word’s meaning doesn’t depend heavily on distant context (e.g., function words like “the”, “a”, “is”), Γ_r 
            can be small, allowing the GRU to reset and focus primarily on the new input information.

            For long-term dependencies, such as in the sentence:
                "The cat ... was full",
            Γ_r stays higher at relevant steps so that past information about “cat” can flow forward into the prediction of “was”.

            In essence, Γ_r acts as a context filter, controlling which parts of the previous hidden state remain influential when computing the new 
            candidate memory c̃<t>.

    Update Gate (Γ_u):
        The update gate Γ_u decides how much of the old memory cell c<t-1> to keep vs. how much of the new candidate c~<t> to use.
        Formula:
            Γ_u = σ(W_u * [a<t-1>, x<t>] + b_u)
        where σ is the sigmoid function.

    Memory Cell Update Equation:
        The new memory cell is a weighted combination of the old cell and the candidate:
            c<t> = Γ_u * c<t-1> + (1 - Γ_u) * c~<t>
        If Γ_u is close to 1 → keep the old memory cell.
        If Γ_u is close to 0 → replace with the candidate value.

    Example Sentence:
        Consider: "The cat, which already ate..., was full."

        To connect "cat" to "was", we want the memory cell holding the fact that the subject is “cat” to be preserved for many time steps 
        (through the clause “which already ate...”).

        For those intermediate steps, the update gate Γ_u should be very close to 1 for the part of the memory holding the subject, meaning:
            c<t> ≈ c<t-1>
        This way, the information flows unchanged across time, avoiding vanishing gradients.

        For other parts of the cell that hold irrelevant information for this step, Γ_u can be set close to 0, allowing updates from c~<t>.
        Choosing Γ_u very close to 1 for important information allows it to travel far down the sequence without much loss.

        Reset Gate (Γ_r):
            The reset gate decides how much past information to consider when calculating the candidate cell c~<t>:
                Γ_r = σ(W_r * [a<t-1>, x<t>] + b_r)
            If Γ_r is small → the GRU “forgets” the old hidden state and focuses on the current input.

            If Γ_r is large → the GRU uses the old hidden state heavily to compute the candidate.

        Full GRU Equations:
            Update Gate:
                Γ_u = σ(W_u * [a<t-1>, x<t>] + b_u)
            Reset Gate:
                Γ_r = σ(W_r * [a<t-1>, x<t>] + b_r)
            Candidate Memory Cell:
                c~<t> = tanh(W_c * [Γ_r * a<t-1>, x<t>] + b_c)
            Final Memory Cell:
                c<t> = Γ_u * c<t-1> + (1 - Γ_u) * c~<t>
            Hidden State:
            In a GRU, the hidden state a<t> is the same as the memory cell:
                a<t> = c<t>

        This gating mechanism makes GRUs much better than vanilla RNNs at remembering important information over long time spans, while still being 
        simpler than LSTMs (which have separate cell and hidden states).

Long Short Term Memory (LSTMs):
    LSTM (Long Short-Term Memory) networks were introduced to address the vanishing and exploding gradient problems of basic RNNs, enabling models 
    to capture long-term dependencies in sequential data. Unlike standard RNNs, which overwrite their hidden state at each timestep, LSTMs maintain 
    a separate memory cell c<t> that can preserve information over long time spans. This is achieved using three gates—update (input) gate, 
    forget gate, and output gate—that control what information is stored, discarded, or output from the memory cell. LSTMs are particularly effective 
    in tasks where context from far in the past is crucial, such as machine translation, text generation, and speech recognition.
        
    Formulas for LSTM:
        Candidate cell state:
            c_tilde<t> = tanh(W_c [a<t-1>, x<t>] + b_c)

        Update (input) gate:
            Γ_u = σ(W_u [a<t-1>, x<t>] + b_u)

        Forget gate:
            Γ_f = σ(W_f [a<t-1>, x<t>] + b_f)

        Output gate:
            Γ_o = σ(W_o [a<t-1>, x<t>] + b_o)

        Cell state update:
            c<t> = Γ_f ⊙ c<t-1> + Γ_u ⊙ c_tilde<t>

        Activation output:
            a<t> = Γ_o ⊙ tanh(c<t>)

            Where:
                σ is the sigmoid function, outputs values between 0 and 1.
                ⊙ denotes element-wise multiplication.
                [a<t-1>, x<t>] is the concatenation of the previous activation and the current input.

Architecture of LSTM:

            ┌───────────┐
   a<t-1>──▶│           │──┐
            │ Forget    │  │
   x<t>  ──▶│ Gate Γ_f  │  │
            └───────────┘  │
                           ▼
                        c<t-1> ──┐
                                  \
                                   ⊙───┐
                                       ▼
            ┌───────────┐         ┌───────────┐
   a<t-1>──▶│ Update    │──⊙────▶ │           │
            │ Gate Γ_u  │         │ Candidate │
   x<t>  ──▶│           │         │ c_tilde<t>│
            └───────────┘         └───────────┘
                                       │
                                       ▼
                                     ( + )──▶ c<t>
                                       │
            ┌───────────┐             ▼
   a<t-1>──▶│ Output    │──⊙──▶ a<t>
            │ Gate Γ_o  │
   x<t>  ──▶│           │
            └───────────┘

    Parallel connections:
        The cell state path (c<t>) runs horizontally across timesteps with minimal modification, allowing gradients to pass without vanishing.
        Gates control how much of the past flows forward and how much new information is written in.
        This “constant error carousel” design makes it easy to trace back which earlier inputs are influencing current outputs.

        Example sentence:
            The cat, which had eaten too much, was sleeping.
            Let’s say we’re at the token "was".
            The LSTM’s cell state c<t> has been carrying subject information from "The cat" through many timesteps.
            At each step, the forget gate Γ_f mostly stays high (≈1) for the “subject-tracking” dimensions, so the subject info is preserved.
            For unrelated details like "eaten" or "too much", Γ_f is lower, letting that info decay.
            By the time we reach "was", the output gate Γ_o lets through the part of c<t> that signals "singular subject", guiding the network to choose 
            "was" instead of "were".

            Here, the “parallel connection” of c<t> from "The" all the way to "was" is what lets the model bridge that long gap without forgetting the subject’s number.

    Uses and benefits:
        Can model long-term dependencies without severe gradient decay.
        Works well in natural language processing, time-series forecasting, and speech synthesis.
        Flexible gating system allows selective memory retention and forgetting.
        Helps stabilize training in long sequences.

    Comparison with GRU:
        GRU combines update and forget gates into one, making it simpler and faster to train.
        LSTM has separate gates, offering finer control over memory but with more parameters.
        GRUs can perform similarly on many tasks, but LSTMs tend to win in very long dependency scenarios.

    Conclusion:
        LSTMs remain one of the most influential architectures in deep learning for sequence modeling. By explicitly managing how information flows 
        and persists through time, they overcome fundamental limitations of vanilla RNNs. While newer architectures like Transformers dominate in many 
        NLP tasks today, LSTMs still excel in smaller datasets, real-time applications, and domains where their inductive biases about sequential 
        memory are advantageous.